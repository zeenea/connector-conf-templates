name = ""
code = ""
connector_id = "AmazonS3"
enabled = true

# Define the catalog code associated with the connection ("default" when empty).
# catalog_code = ""

# Defines the list of aliases used by other connectors to generate lineage link.
# alias = ["localhost:1234/db","https://some-url.org"]

# Define secret manager for this connection. 
# This configuration only works with Scanner 73+ and needs a functional secret manager configured in the scanner configuration file.
#secret_manager {
#    enabled = true
#    key = "<secret_name>"
#}

connection {
    # The connector uses the official Amazon SDK.
    # So the following parameters can be set to specify a region and an access key.
    # If not set, information will be taken from:
    #   1. environment variables
    #   2. shared credential and config files
    #   3. ECS container or EC2 instance role.
    #
    # see Amazon documentation <https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/credentials.html>
    #
    aws {
        # access_key_id = 
        # secret_access_key =
        # region = "eu-west-3"
    }

    # Custom URL (only for S3 compatible stores)
    #url = null
}

# Proxy configuration
#proxy {
#    # Endpoint URL of the proxy
#    endpoint = null
#
#    # proxy user name
#    username = null
#    
#    # proxy user password
#    password = null
#}

# List of buckets to Bucket List (spaces separated)
#s3.bucket_list = null

# Cache
# When inventoried buckets are very large and contains a lot of objects, the connector can consume a large amount of memory.
# It is possible to cache the object list on disque to reduce memory consumption.
# To enable disk cache, just set the path to a file. (You can both set an absolute or a relative path, however, relative paths are dependant of the current directory at the scanner launch time.
# Examples:
#     - "/opt/zeenea-scanner/cache/s3.cache"
#     - "/var/lib/zeenea-scanner/s3.cache"
#     - """C:\zeenea-scanner\cache\s3.cache"""
# cache {
#     path = null
# }

xml {
    # Use new XML namespace identification, replace with value "legacy" if XML were imported from the connection before.
    namespace_identification = "uri"

    # XML field ordering. Accepted values :
    #   * "alphabetical"                : Fields are ordered alphabetically
    #   * "", "legacy" or "unordered"   : Fields are ordered as they are read in the XML file, used before version 67
    # By default, fields are ordered as they are read in the XML file (unordered).
    # fields_ordering = ""
}

# Filter
#
# Filter the objects that should be taken into consideration by the inventory.
#
# Use the rich filter language describe in documentation.
#       English version: https://support.zeenea.com/hc/en/articles/21269299175569-Filtres
#       French  version: https://support.zeenea.com/hc/fr/articles/21269299175569-Filtres
#
# filter keys are:
#   bucket: the bucket name.
#   key   : the key of the object in the bucket.
#
# If there is an object 's3:///cogip_prod_sales/forecasts/europa/france/paris-2023.parquet'.
# The value of associated to 'bucket' will be 'cogip_prod_sales'.
# The value of associated to 'key' will be 'forecasts/europa/france/paris-2023.parquet'.
#
# Example:
#   filter = """
#           bucket starts with 'cogip_prod_'
#       and ! (
#              key ~ '*/test*'
#           or key ~ /.*\/secret_assets\/.*/
#           )
#   """
#
# Note: Filter can contain either the raw value or a file URL to the content.
# Example:
#   filter = "file:///path/to/zeenea/connections/s3-filter.json"
#
# When using an side-file, filter changes are taken into account without restarting the scanner.
#
# filter = ""

#
# Inventory section is part of the advanced configuration.
# It allow to fine tune the behaviour of the inventory and dataset discovery.
#
# It should only be touched with a trembling hand.
#
inventory {
    # Inventory strategy.
    #
    # Choose the algorithm used to discover datasets.
    # Two possible values: "standard" or "legacy" (default).
    # Legacy strategy is the algorithm used before version 59.
    # Standard strategy can manage the case when several datasets lie in the same folder.
    strategy = "standard"

    # With standard strategy, the file partition pattern define the changing part of a file.
    # The parts of the file name without the extension that match the pattern are replaced by a start to form the sybolic name of the  
    # For instance, if you have the given files:
    #       * some/path/to/sales/customers-2022.parquet
    #       * some/path/to/sales/customers-2023.parquet
    #       * some/path/to/sales/sales-2022.parquet
    #       * some/path/to/sales/sales-2023.parquet
    #
    # We can set the file partition pattern to be four digits at the end of the file name.
    # inventory.file_partition_pattern = """\d{4}$""" 
    #
    # In that case two datasets:
    #       * some/path/to/sales/customers-*.parquet
    #       * some/path/to/sales/sales-*.parquet
    #
    # The default value matches:
    #       * spark partion file names /^part-[-0-9a-fA-F]+$/
    #       * 8 digits date at the end of the file name, with /\d{4}-\d{2}-\d{2}$/ or without dashes /\d{8}$/.
    #
    #file_partition_pattern = """(?:^part-[-0-9a-fA-F]+|\d{4}(?:\d{4}|-\d{2}-\d{2}))$"""

    #
    # Pattern of the name of the folders that are a partition of a dataset.
    #
    # Default value is equivalent to:
    #     .*=.*      # Key=Value Partition
    #   | \d{8}      # ISO Date partition
    #   | \d{4}      # Year Partition
    #   | \d\d?      # Day or Month Partition
    #
    #partition = """.*=.*|\d(?:\d(?:\d{2}(?:\d{4})?)?)?"""

    #
    # Pattern of the name of the folders to skip while keeping the content taken into account. The content will be scanned as if it were at the root of the parent folder.
    #
    # The behavior is technically equivalent to considering it as a partition but it has another meaning.
    # It is typically used when there is a dummy folder in the path.
    #
    # For instance, by skipping the data folder, the following dataset will be identified as "sales" instead of "data":
    #       * some/path/to/sales/data/2022/customers-202212.parquet
    #       * some/path/to/sales/data/2023/customers-202301.parquet
    #       * some/path/to/sales/data/2023/customers-202302.parquet
    #
    # No folder is skipped by default.
    #
    #skippedDirectory = ""

    #
    # Pattern of the name of the folders to ignore during the detection of the.
    #
    # It can be used with skippedDirectory to ignore techical folders in a dataset.
    #
    # For instance, the "sales" dataset contains a folder "data" and a folder "logs" with data processing logs.
    # For this dataset to be correctly discovered, we need to skip "data" and ignore "logs".
    #       * some/path/to/sales/data/2022/customers-202212.parquet
    #       * some/path/to/sales/data/2023/customers-202301.parquet
    #       * some/path/to/sales/data/2023/customers-202302.parquet
    #       * some/path/to/sales/logs/customers-creation-202212.parquet
    #       * some/path/to/sales/logs/customers-creation-202301.parquet
    #       * some/path/to/sales/logs/customers-creation-202302.parquet
    #
    # No folder is ignored by default.
    #
    #ignoredDirectory = ""

    # Ignored files during dataset detection inventory.
    #
    #ignoredFile = ""

    # Used for configuring csv files header detection pattern. 
    #
    # SHOULD NOT BE CHANGED AFTER ITEMS HAVE BEEN IMPORTED OR SOME FIELDS IDENTIFIER WILL CHANGE.
    #
    # Select "always" to force recognizing the schema on the first line of csv files. 
    # Possible values are: "never", "default", "only string" and "always".
    #   * never       : The first line always seen as a line of data.
    #   * default     : The first line is a header if at least one column has a different type in the other lines.
    #   * only string : The first line is a header if it contains only string values. 
    #   * always      : The first line is always considered to be header.
    #
    # The default value is "default".
    #
    #csv.header = ""
}
